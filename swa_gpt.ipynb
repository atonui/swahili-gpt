{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA2EdwVwf+m+rWA5vDGXa2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atonui/swahili-gpt/blob/main/swa_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swahili-GPT\n",
        "- A simple character level transformer based LLM trained on a Swahili dataset."
      ],
      "metadata": {
        "id": "nZvhfs91jrcf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Co39Fg4x5Z",
        "outputId": "1cd35510-93e5-4779-d1ff-2ce67b1a8cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mpwolke/swahili?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.65M/2.65M [00:00<00:00, 24.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mpwolke/swahili/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mpwolke/swahili\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# https://www.kaggle.com/datasets/mpwolke/swahili/code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the training dataset\n",
        "!wget https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-yFDZfe5QZ9",
        "outputId": "3d176d58-3908-4b8c-b245-3bcf23efd575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-19 14:06:48--  https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7658045 (7.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   7.30M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-19 14:06:49 (62.1 MB/s) - ‘train.txt’ saved [7658045/7658045]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it to inspect it\n",
        "with open('train.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "UhB5WPU47Ou7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length if dataset in characters: ', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UxSd4krhuOa",
        "outputId": "4f4cd0b4-b7e2-40b1-99c5-ec442ca407ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length if dataset in characters:  7658045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRigK2Eph4gQ",
        "outputId": "ab8e2675-90a6-4b1a-f5a1-7b5bd3006cae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " taarifa hiyo ilisema kuwa ongezeko la joto la maji juu ya wastani katikati ya bahari ya UNK inaashiria kuwepo kwa mvua za el nino UNK hadi mwishoni mwa april ishirini moja sifuri imeelezwa kuwa ongezeko la joto magharibi mwa bahari ya hindi linatarajiwa kuhamia katikati ya bahari hiyo hali ambayo itasababisha pepo kutoka kaskazini mashariki kuvuma kuelekea bahari ya hindi \n",
            " aidha ilisema kuwa mwelekeo wa kupungua kwa joto kusini mashariki mwa bahari ya atlantic UNK kusababisha pepo kutoka magharibi kuvuma kuelekea magharibi mwa tanzania katika maeneo ya ziwa victoria \n",
            " mwelekeo wa mvua wa septemba hadi desemba ishirini sifuri tisa unatarajiwa kuwa katika namna tofauti ambapo baadhi ya maeneo yanaweza kunufaika huku mengine UNK \n",
            " ilifafanua kuwa msimu wa vuli UNK maeneo ambayo hupata mvua mara mbili ambayo ni kaskazini mwa nchi ikiwa ni nyanda za juu kaskazini mashariki kanda ya ziwa victoria na pwani ya kaskazini \n",
            " katika maeneo hayo mvua zinatarajiwa kunyesha wiki ya pili na tatu ya \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a sorted list of all the characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(\"There are\", vocab_size, \"unique characters in this text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRhQYJhSh8q4",
        "outputId": "9cea3c43-305a-4b31-8826-4907f3725ccb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " KNUabcdefghijklmnopqrstuvwxyz\n",
            "There are 31 unique characters in this text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation\n",
        "- Represent characters as integers (vectors) so the model can manipulate them.\n",
        "- The below tokeniser is simple, it just translates a character to an integer.\n",
        "- There are more sophisticated tokenisers out there, we shall experiment with them."
      ],
      "metadata": {
        "id": "YJyI7xQQimZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"joto jingi\"))\n",
        "print(decode(encode(\"joto jingi\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEcJKxRMiXo4",
        "outputId": "7c098ec9-9720-4840-9379-9106bec8d863"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 19, 24, 19, 1, 14, 13, 18, 11, 13]\n",
            "joto jingi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode([14, 19, 24, 19, 1, 16, 13, 18, 11, 13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSibstVvl-19",
        "outputId": "65dee3d9-6ec4-401a-f63e-b6996223cba8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joto lingi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we're going to encode the entire dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "train_data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(train_data.shape, train_data.type)\n",
        "print(train_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzexWCVBnuuK",
        "outputId": "75cc2f42-16b0-4c4a-eed0-750c664f6aad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7658045]) <built-in method type of Tensor object at 0x7f8b721fcd70>\n",
            "tensor([ 1, 24,  5,  5, 22, 13, 10,  5,  1, 12, 13, 29, 19,  1, 13, 16, 13, 23,\n",
            "         9, 17,  5,  1, 15, 25, 27,  5,  1, 19, 18, 11,  9, 30,  9, 15, 19,  1,\n",
            "        16,  5,  1, 14, 19, 24, 19,  1, 16,  5,  1, 17,  5, 14, 13,  1, 14, 25,\n",
            "        25,  1, 29,  5,  1, 27,  5, 23, 24,  5, 18, 13,  1, 15,  5, 24, 13, 15,\n",
            "         5, 24, 13,  1, 29,  5,  1,  6,  5, 12,  5, 22, 13,  1, 29,  5,  1,  4,\n",
            "         3,  2,  1, 13, 18,  5,  5, 23, 12, 13, 22, 13,  5,  1, 15, 25, 27,  9,\n",
            "        20, 19,  1, 15, 27,  5,  1, 17, 26, 25,  5,  1, 30,  5,  1,  9, 16,  1,\n",
            "        18, 13, 18, 19,  1,  4,  3,  2,  1, 12,  5,  8, 13,  1, 17, 27, 13, 23,\n",
            "        12, 19, 18, 13,  1, 17, 27,  5,  1,  5, 20, 22, 13, 16,  1, 13, 23, 12,\n",
            "        13, 22, 13, 18, 13,  1, 17, 19, 14,  5,  1, 23, 13, 10, 25, 22, 13,  1,\n",
            "        13, 17,  9,  9, 16,  9, 30, 27,  5,  1, 15, 25, 27,  5,  1, 19, 18, 11,\n",
            "         9, 30,  9, 15, 19,  1, 16,  5,  1, 14, 19, 24, 19,  1, 17,  5, 11, 12,\n",
            "         5, 22, 13,  6, 13,  1, 17, 27,  5,  1,  6,  5, 12,  5, 22, 13,  1, 29,\n",
            "         5,  1, 12, 13, 18,  8, 13,  1, 16, 13, 18,  5, 24,  5, 22,  5, 14, 13,\n",
            "        27,  5,  1, 15, 25, 12,  5, 17, 13,  5,  1, 15,  5, 24, 13, 15,  5, 24,\n",
            "        13,  1, 29,  5,  1,  6,  5, 12,  5, 22, 13,  1, 12, 13, 29, 19,  1, 12,\n",
            "         5, 16, 13,  1,  5, 17,  6,  5, 29, 19,  1, 13, 24,  5, 23,  5,  6,  5,\n",
            "         6, 13, 23, 12,  5,  1, 20,  9, 20, 19,  1, 15, 25, 24, 19, 15,  5,  1,\n",
            "        15,  5, 23, 15,  5, 30, 13, 18, 13,  1, 17,  5, 23, 12,  5, 22, 13, 15,\n",
            "        13,  1, 15, 25, 26, 25, 17,  5,  1, 15, 25,  9, 16,  9, 15,  9,  5,  1,\n",
            "         6,  5, 12,  5, 22, 13,  1, 29,  5,  1, 12, 13, 18,  8, 13,  1,  0,  1,\n",
            "         5, 13,  8, 12,  5,  1, 13, 16, 13, 23,  9, 17,  5,  1, 15, 25, 27,  5,\n",
            "         1, 17, 27,  9, 16,  9, 15,  9, 19,  1, 27,  5,  1, 15, 25, 20, 25, 18,\n",
            "        11, 25,  5,  1, 15, 27,  5,  1, 14, 19, 24, 19,  1, 15, 25, 23, 13, 18,\n",
            "        13,  1, 17,  5, 23, 12,  5, 22, 13, 15, 13,  1, 17, 27,  5,  1,  6,  5,\n",
            "        12,  5, 22, 13,  1, 29,  5,  1,  5, 24, 16,  5, 18, 24, 13,  7,  1,  4,\n",
            "         3,  2,  1, 15, 25, 23,  5,  6,  5,  6, 13, 23, 12,  5,  1, 20,  9, 20,\n",
            "        19,  1, 15, 25, 24, 19, 15,  5,  1, 17,  5, 11, 12,  5, 22, 13,  6, 13,\n",
            "         1, 15, 25, 26, 25, 17,  5,  1, 15, 25,  9, 16,  9, 15,  9,  5,  1, 17,\n",
            "         5, 11, 12,  5, 22, 13,  6, 13,  1, 17, 27,  5,  1, 24,  5, 18, 30,  5,\n",
            "        18, 13,  5,  1, 15,  5, 24, 13, 15,  5,  1, 17,  5,  9, 18,  9, 19,  1,\n",
            "        29,  5,  1, 30, 13, 27,  5,  1, 26, 13,  7, 24, 19, 22, 13,  5,  1,  0,\n",
            "         1, 17, 27,  9, 16,  9, 15,  9, 19,  1, 27,  5,  1, 17, 26, 25,  5,  1,\n",
            "        27,  5,  1, 23,  9, 20, 24,  9, 17,  6,  5,  1, 12,  5,  8, 13,  1,  8,\n",
            "         9, 23,  9, 17,  6,  5,  1, 13, 23, 12, 13, 22, 13, 18, 13,  1, 23, 13,\n",
            "        10, 25, 22, 13,  1, 24, 13, 23,  5,  1, 25, 18,  5, 24,  5, 22,  5, 14,\n",
            "        13, 27,  5,  1, 15, 25, 27,  5,  1, 15,  5, 24, 13, 15,  5,  1, 18,  5,\n",
            "        17, 18,  5,  1, 24, 19, 10,  5, 25, 24, 13,  1,  5, 17,  6,  5, 20, 19,\n",
            "         1,  6,  5,  5,  8, 12, 13,  1, 29,  5,  1, 17,  5,  9, 18,  9, 19,  1,\n",
            "        29,  5, 18,  5, 27,  9, 30,  5,  1, 15, 25, 18, 25, 10,  5, 13, 15,  5,\n",
            "         1, 12, 25, 15, 25,  1, 17,  9, 18, 11, 13, 18,  9,  1,  4,  3,  2,  1,\n",
            "         0,  1, 13, 16, 13, 10,  5, 10,  5, 18, 25,  5,  1, 15, 25, 27,  5,  1,\n",
            "        17, 23, 13, 17, 25,  1, 27,  5,  1, 26, 25, 16, 13,  1,  4,  3,  2,  1,\n",
            "        17,  5,  9, 18,  9, 19,  1,  5, 17,  6,  5, 29, 19,  1, 12, 25, 20,  5,\n",
            "        24,  5,  1, 17, 26, 25,  5,  1, 17,  5, 22,  5,  1, 17,  6, 13, 16, 13,\n",
            "         1,  5, 17,  6,  5, 29, 19,  1, 18, 13,  1, 15,  5, 23, 15,  5, 30, 13,\n",
            "        18, 13,  1, 17, 27,  5,  1, 18,  7, 12, 13,  1, 13, 15, 13, 27,  5,  1,\n",
            "        18, 13,  1, 18, 29,  5, 18,  8,  5,  1, 30,  5,  1, 14, 25, 25,  1, 15,\n",
            "         5, 23, 15,  5, 30, 13, 18, 13,  1, 17,  5, 23, 12,  5, 22, 13, 15, 13,\n",
            "         1, 15,  5, 18,  8,  5,  1, 29,  5,  1, 30, 13, 27,  5,  1, 26, 13,  7,\n",
            "        24, 19, 22, 13,  5,  1, 18,  5,  1, 20, 27,  5, 18, 13,  1, 29,  5,  1,\n",
            "        15,  5, 23, 15,  5, 30, 13, 18, 13,  1,  0,  1, 15,  5, 24, 13, 15,  5,\n",
            "         1, 17,  5,  9, 18,  9, 19,  1, 12,  5, 29, 19,  1, 17, 26, 25,  5,  1,\n",
            "        30, 13, 18,  5, 24,  5, 22,  5, 14, 13, 27,  5,  1, 15, 25, 18, 29,  9,\n",
            "        23, 12,  5,  1, 27, 13, 15, 13,  1, 29,  5,  1, 20, 13, 16, 13,  1, 18,\n",
            "         5,  1, 24,  5, 24, 25,  1, 29,  5,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset\n",
        "!wget https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/test.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_LNLRnar98n",
        "outputId": "89ec6d8b-53d4-4ba1-e666-d798e54b3ecc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-19 14:06:55--  https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691715 (676K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 675.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-08-19 14:06:55 (16.9 MB/s) - ‘test.txt’ saved [691715/691715]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it to inspect it\n",
        "with open('test.txt', 'r') as f:\n",
        "    test = f.read()"
      ],
      "metadata": {
        "id": "xuWYhPOLwF0T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode test dataset into a tensor\n",
        "test_data = torch.tensor(encode(test), dtype=torch.long)\n",
        "print(test_data.shape, test_data.type)\n",
        "print(test_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMVNx7fVz_0_",
        "outputId": "c76c1312-e8f7-4135-96a4-b2e3db4478ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([691715]) <built-in method type of Tensor object at 0x7f8aa8d4ca10>\n",
            "tensor([ 1, 12, 25, 29, 19,  1,  5, 16, 13, 23, 13, 23, 13, 24, 13, 30,  5,  1,\n",
            "        15, 25, 27,  5,  1, 12,  5, 15, 25, 12, 19, 14, 13, 27,  5,  1,  6,  5,\n",
            "        16, 13,  1,  5, 16, 13, 20,  9, 27,  5,  1, 15,  5, 22,  5, 24,  5, 23,\n",
            "        13,  1, 24, 25, 20, 25,  1, 18,  5,  1, 15, 25, 24,  5, 15, 13, 27,  5,\n",
            "         1, 15, 25, 23,  5, 13, 18, 13,  1,  6, 13, 16,  5,  1, 15, 25, 10,  5,\n",
            "        12,  5, 17, 25,  1, 18, 13,  1, 15, 13, 24, 25,  1, 11,  5, 18, 13,  1,\n",
            "         4,  3,  2,  1,  0,  1,  5, 15, 13,  9, 16,  9, 30,  9,  5,  1, 23, 13,\n",
            "        15, 25,  1, 29,  5,  1, 24, 25, 15, 13, 19,  1,  5, 16, 13, 23,  9, 17,\n",
            "         5,  1,  5, 16, 13, 20, 13, 11, 13, 27,  5,  1, 23, 13, 17, 25,  1, 18,\n",
            "         5,  1, 23, 23, 20,  1, 23,  5, 16, 25, 17,  1, 15, 13, 23,  5, 13,  1,\n",
            "         5, 17,  6,  5, 29,  9,  1,  5, 16, 13, 17, 24,  5, 15,  5,  1, 15, 25,\n",
            "        10, 13, 15,  5,  1, 15,  5, 24, 13, 15,  5,  1, 19, 10, 13, 23, 13,  1,\n",
            "        30,  5,  1, 24, 25, 17,  9,  1, 12, 13, 29, 19,  1, 18,  5,  1,  5, 16,\n",
            "        13, 10, 13, 15,  5,  1, 12,  5, 20, 19,  1, 19, 15, 24, 19,  6,  5,  1,\n",
            "        15, 25, 17, 13,  1, 17, 27,  5, 15,  5,  1, 14,  5, 18,  5,  1, 15, 25,\n",
            "        24,  9, 15,  9, 16,  9, 30,  5,  1,  5, 11, 13, 30, 19,  1, 16,  5,  1,\n",
            "        17, 20,  9, 16,  9, 16,  9, 30, 13,  1, 12, 25, 29, 19,  1,  0,  1,  5,\n",
            "        16, 13,  8,  5, 13,  1, 15, 25, 27,  5,  1,  6,  5,  5,  8,  5,  1, 29,\n",
            "         5,  1, 15, 25, 10, 13, 15,  5,  1, 12,  5, 20, 19,  1,  5, 16, 13,  5,\n",
            "        17,  6, 13, 27,  5,  1, 18,  5,  1,  6, 27,  5, 18,  5,  1, 15, 13, 23,\n",
            "         5, 13,  1, 15, 25, 27,  5,  1,  5, 18,  5, 12, 13, 24,  5, 14, 13,  1,\n",
            "        15, 25, 10,  5, 18, 29,  5,  1, 17,  5, 22,  9, 15,  9,  6, 13, 23, 12,\n",
            "        19,  1, 17,  5,  7, 12,  5,  7, 12,  9,  1, 15, 27,  9, 18, 29,  9,  1,\n",
            "        17,  5,  9, 16,  9, 30, 19,  1, 29,  5,  1, 24, 25, 12, 25, 17,  5,  1,\n",
            "        30,  5, 15,  9,  1, 12, 13, 26, 29, 19,  1,  5, 16, 13, 17, 20,  5,  1,\n",
            "        15,  5, 22,  5, 24,  5, 23, 13,  1, 24, 25, 20, 25,  1,  4,  3,  2,  1,\n",
            "        18,  5,  1,  6,  5,  5,  8,  5, 29,  9,  1, 17, 20,  9, 16,  9, 16,  9,\n",
            "        30, 13,  1, 12, 25, 29, 19,  1,  5, 18, 11,  9,  5, 18,  8, 13, 15,  5,\n",
            "         1, 17,  5,  9, 16,  9, 30, 19,  1,  0,  1,  5, 16, 13,  8,  5, 13,  1,\n",
            "        15, 25, 27,  5,  1, 15,  5,  6, 16,  5,  1, 29,  5,  1, 15, 25, 24, 13,\n",
            "         5,  1, 23,  5, 13, 18, 13,  1,  5, 16, 13, 22, 25, 12, 25, 23, 13, 27,\n",
            "         5,  1, 15, 27,  9, 18,  8,  5,  1, 17, 23, 13, 15, 13, 24, 13, 18, 13,\n",
            "         1, 15, 25, 23, 27,  5, 16, 13,  1, 12, 13, 26, 29, 19,  1,  5, 16, 13,\n",
            "        19, 18,  8, 19, 15,  5,  1, 15, 25,  9, 16,  9, 15,  9,  5,  1, 17,  5,\n",
            "         9, 18,  9, 19,  1, 29,  5,  1, 17, 13, 15, 19,  7, 12,  9, 18, 13,  1,\n",
            "         8,  5, 22,  1,  9, 23,  1, 23,  5, 16,  5,  5, 17,  1,  0,  1, 23, 13,\n",
            "        24,  5, 23,  5, 12,  5, 25,  1, 15,  5, 24, 13, 15,  5,  1, 17,  5, 13,\n",
            "        23, 12,  5,  1, 29,  5, 18, 11, 25,  1, 18, 13,  1, 24, 25, 15, 13, 19,\n",
            "         1, 16,  5,  1,  5, 14,  5,  6, 25,  1,  5, 17,  6,  5, 16, 19,  1, 18,\n",
            "        13, 16, 13, 10,  5, 18, 29, 13, 27,  5,  1, 18, 13, 15, 13, 27,  5,  1,\n",
            "        17, 23, 13, 15, 13, 24, 13, 18, 13,  1,  5, 16, 13, 15, 25, 14,  5,  1,\n",
            "        19, 10, 13, 23,  5,  1, 17, 17, 19, 14,  5,  1, 27,  5,  1, 24, 25, 17,\n",
            "         9,  1, 18,  5,  1, 15, 25, 18, 13, 24,  5, 15,  5,  1, 18, 13,  9, 18,\n",
            "         8,  9,  1, 19, 10, 13, 23, 13,  1, 30,  5, 19,  1, 12, 25, 15, 25,  1,\n",
            "        17, 24, 19,  5,  1, 12, 19, 24, 25,  6,  5,  1,  5, 15, 13, 27,  5,  1,\n",
            "         5, 17,  9, 20,  5, 18,  8,  5,  1,  9, 18,  9, 19,  1, 16,  5,  1, 15,\n",
            "        25, 24, 19, 16,  9,  5,  1, 12, 19, 24, 25,  6,  5,  1, 12, 13, 29, 19,\n",
            "         1, 15, 27,  5,  1, 27,  5, 13, 23, 16,  5, 17,  1, 18,  8, 13,  7, 12,\n",
            "        19,  1, 15, 13, 24, 25,  1, 17, 25, 12, 13, 17, 25,  1, 23,  5, 18,  5,\n",
            "         1,  5, 16, 13,  8,  5, 13,  1,  6, 27,  5, 18,  5,  1, 10,  5, 22, 13,\n",
            "        14,  5, 16,  5,  1,  0,  1,  5, 16, 13,  8,  5, 13,  1, 15, 25, 27,  5,\n",
            "         1,  6,  5,  5,  8,  5,  1, 29,  5,  1, 15, 25, 10, 13, 15,  5,  1, 15,\n",
            "         5, 24, 13, 15,  5,  1, 24, 25, 17,  9,  1, 12, 13, 29, 19,  1,  5, 16,\n",
            "        13, 20,  9, 27,  5,  1, 15,  5, 22,  5, 24,  5, 23, 13,  1, 30, 13, 16,\n",
            "         9,  1, 30, 13, 16,  9,  1, 18,  5,  1, 15, 25,  5, 17,  6, 13, 27,  5,\n",
            "         1,  5, 23,  5, 13, 18, 13,  1, 13, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation dataset\n",
        "!wget https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/valid.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn8P7PFyxfkp",
        "outputId": "709fd12e-c166-4f91-93d9-fdb177e89ec8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-19 14:06:55--  https://raw.githubusercontent.com/atonui/pds/refs/heads/main/Swahili%20data/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 660142 (645K) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>] 644.67K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-08-19 14:06:56 (18.5 MB/s) - ‘valid.txt’ saved [660142/660142]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it to inspect it\n",
        "with open('valid.txt', 'r') as f:\n",
        "    valid = f.read()"
      ],
      "metadata": {
        "id": "zWlkO498xmlh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode validation dataset into a tensor\n",
        "valid_data = torch.tensor(encode(valid), dtype=torch.long)\n",
        "print(valid_data.shape, valid_data.type)\n",
        "print(valid_data[:100])\n",
        "\n",
        "# code from the repetitive cells above is ripe for a function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eN33UQOxtW4",
        "outputId": "d322a91b-c0e2-4ae0-a5cc-75a818e114b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([660142]) <built-in method type of Tensor object at 0x7f8b723c38f0>\n",
            "tensor([ 1, 12, 13, 13,  1, 18, 13,  1,  8, 12,  5, 18,  5,  1, 20, 19, 24, 19,\n",
            "        10, 25,  1, 18,  5,  1, 29,  5,  1, 12,  5, 24,  5, 22, 13,  1, 12,  5,\n",
            "        23,  5,  1, 25, 15, 13, 30, 13, 18, 11,  5, 24, 13,  5,  1,  6,  5,  5,\n",
            "         8, 12, 13,  1, 29,  5,  1, 27,  5, 18,  5, 18,  7, 12, 13,  1, 27,  9,\n",
            "        18, 11, 13,  1, 27,  5, 18,  5,  1, 17,  5, 23, 12,  5, 15,  5,  1, 18,\n",
            "         5,  1, 25, 30,  5, 16,  9, 18,  8, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training block size\n",
        "block_size = 8\n",
        "train_data[:block_size+1]\n",
        "# the transformer is not trained on the entire text but on blocks of text e.g. the above block size is 9 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8zSalDTzo_p",
        "outputId": "6f459f74-6529-49f4-fdf6-f124ea1092a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1, 24,  5,  5, 22, 13, 10,  5,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # inputs to the transformer\n",
        "y = train_data[1:block_size+1] # next block size, it is offset by 1\n",
        "# iterating through the block size\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'when input is {context} the target: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeviTeJVzo86",
        "outputId": "510e807c-df1a-440b-9715-263bd77cd0d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([1]) the target: 24\n",
            "when input is tensor([ 1, 24]) the target: 5\n",
            "when input is tensor([ 1, 24,  5]) the target: 5\n",
            "when input is tensor([ 1, 24,  5,  5]) the target: 22\n",
            "when input is tensor([ 1, 24,  5,  5, 22]) the target: 13\n",
            "when input is tensor([ 1, 24,  5,  5, 22, 13]) the target: 10\n",
            "when input is tensor([ 1, 24,  5,  5, 22, 13, 10]) the target: 5\n",
            "when input is tensor([ 1, 24,  5,  5, 22, 13, 10,  5]) the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else valid_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs: ')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets: ')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----------')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "  for t in range(block_size): # time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'when input is {context.tolist()} the target: {target}')"
      ],
      "metadata": {
        "id": "T79l6ytgZ8-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9226cfa1-0a85-44ce-c09a-19067ac83822"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[ 5, 24, 13,  1, 27,  5,  1, 15],\n",
            "        [13,  6, 13, 19,  1, 29,  5, 15],\n",
            "        [ 1, 27,  5, 30, 19,  1, 16,  5],\n",
            "        [13,  1, 12, 13, 16, 19,  1, 16]])\n",
            "targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 13,  1, 27,  5,  1, 15, 25],\n",
            "        [ 6, 13, 19,  1, 29,  5, 15,  9],\n",
            "        [27,  5, 30, 19,  1, 16,  5,  1],\n",
            "        [ 1, 12, 13, 16, 19,  1, 16, 13]])\n",
            "----------\n",
            "when input is [5] the target: 24\n",
            "when input is [5, 24] the target: 13\n",
            "when input is [5, 24, 13] the target: 1\n",
            "when input is [5, 24, 13, 1] the target: 27\n",
            "when input is [5, 24, 13, 1, 27] the target: 5\n",
            "when input is [5, 24, 13, 1, 27, 5] the target: 1\n",
            "when input is [5, 24, 13, 1, 27, 5, 1] the target: 15\n",
            "when input is [5, 24, 13, 1, 27, 5, 1, 15] the target: 25\n",
            "when input is [13] the target: 6\n",
            "when input is [13, 6] the target: 13\n",
            "when input is [13, 6, 13] the target: 19\n",
            "when input is [13, 6, 13, 19] the target: 1\n",
            "when input is [13, 6, 13, 19, 1] the target: 29\n",
            "when input is [13, 6, 13, 19, 1, 29] the target: 5\n",
            "when input is [13, 6, 13, 19, 1, 29, 5] the target: 15\n",
            "when input is [13, 6, 13, 19, 1, 29, 5, 15] the target: 9\n",
            "when input is [1] the target: 27\n",
            "when input is [1, 27] the target: 5\n",
            "when input is [1, 27, 5] the target: 30\n",
            "when input is [1, 27, 5, 30] the target: 19\n",
            "when input is [1, 27, 5, 30, 19] the target: 1\n",
            "when input is [1, 27, 5, 30, 19, 1] the target: 16\n",
            "when input is [1, 27, 5, 30, 19, 1, 16] the target: 5\n",
            "when input is [1, 27, 5, 30, 19, 1, 16, 5] the target: 1\n",
            "when input is [13] the target: 1\n",
            "when input is [13, 1] the target: 12\n",
            "when input is [13, 1, 12] the target: 13\n",
            "when input is [13, 1, 12, 13] the target: 16\n",
            "when input is [13, 1, 12, 13, 16] the target: 19\n",
            "when input is [13, 1, 12, 13, 16, 19] the target: 1\n",
            "when input is [13, 1, 12, 13, 16, 19, 1] the target: 16\n",
            "when input is [13, 1, 12, 13, 16, 19, 1, 16] the target: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "id": "7Gl3V6Y8V--R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316dbe58-ec82-4262-8152-e9b44604791d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5, 24, 13,  1, 27,  5,  1, 15],\n",
            "        [13,  6, 13, 19,  1, 29,  5, 15],\n",
            "        [ 1, 27,  5, 30, 19,  1, 16,  5],\n",
            "        [13,  1, 12, 13, 16, 19,  1, 16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "     # idx and targets are both (B,T) tensor of integers\n",
        "     logits = self.token_embedding_table(idx) # (Batch,Time,Channel) tensor\n",
        "\n",
        "     if targets is None:\n",
        "      loss = None\n",
        "     else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "     return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:,-1,:] # becomes(B,C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B,C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "out = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRNc43wfZYUe",
        "outputId": "a746654e-f7f4-4239-aaf3-b796b390226d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n",
            "tensor(4.2781, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "lmxfiaNwinuxgax meyjomaftuvwt sinywkqjNalpx zm \n",
            "ajfauhgskdhtfmNlpfeyuvghmN jNyUdxnlt rggsnuxsnlprs p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pytorch optimiser object\n",
        "optimiser = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jwxascAYh2tN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimiser.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npOuI2d9u1Lp",
        "outputId": "9151b0d0-6e14-4d1f-b5ed-69823b02a914"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0465545654296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqBCaw_-u1JI",
        "outputId": "22018a85-72c8-44fa-8003-5c17a336296b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " nyeshi kwa fchioa ngweliumu tila ube\n",
            " kjanegezi UNK ki majauru ma kulifaumza i iki zo lmorikuti nani waia hi siletaya kaa yama ku mo tilbeta nsipilisa kwalitarwawaji aona lazwemja sigemkichado ase \n",
            " UNK bana gu wasa ngekwakuchana e sa UNK kurandima \n",
            " ia hari matpuulirugi \n",
            " ay\n",
            " ifazao bo jima yo ku mbwamba he \n",
            " li mbwa h kakali belite pso jumuvyaomatenara itra yafutango hu \n",
            " ku liwa vujayar kwa batileni i wanga iku seadina ta i slileya wa ka dika \n",
            " va unanaziana matu hokubarilosha ndangam si tev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvTH9ATWh2qs"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}